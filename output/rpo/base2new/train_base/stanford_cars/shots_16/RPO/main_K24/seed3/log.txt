***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/RPO/main_K24.yaml
dataset_config_file: configs/datasets/stanford_cars.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output/rpo/base2new/train_base/stanford_cars/shots_16/RPO/main_K24/seed3
resume: 
root: /mnt/ul-nfs-2T/datasets/downstream_data
seed: 3
source_domains: None
target_domains: None
trainer: RPO
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 16
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 4
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: StanfordCars
  NUM_LABELED: -1
  NUM_SHOTS: 16
  PROMPT: a photo of a _.
  ROOT: /mnt/ul-nfs-2T/datasets/downstream_data
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.01
  LR_SCHEDULER: cosine
  MAX_EPOCH: 15
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/rpo/base2new/train_base/stanford_cars/shots_16/RPO/main_K24/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 20
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: fp16
  COOP:
    CLASS_TOKEN_POSITION: 
    CSC: False
    CTX_INIT: 
    N_CTX: 4
    PREC: fp16
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  LP:
    PREC: fp16
    PROMPT: A photo of a {cls_name}
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: RPO
  RPO:
    CTX_INIT: a photo of a
    K: 24
    PREC: fp16
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 2.0.1+cu117
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.4 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
Clang version: Could not collect
CMake version: version 3.27.0
Libc version: glibc-2.31

Python version: 3.9.16 | packaged by conda-forge | (main, Feb  1 2023, 21:39:03)  [GCC 11.3.0] (64-bit runtime)
Python platform: Linux-4.19.93-1.nbp.el7.x86_64-x86_64-with-glibc2.31
Is CUDA available: True
CUDA runtime version: 11.7.99
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: 
GPU 0: NVIDIA A100-SXM4-80GB
GPU 1: NVIDIA A100-SXM4-80GB
GPU 2: NVIDIA A100-SXM4-80GB
GPU 3: NVIDIA A100-SXM4-80GB

Nvidia driver version: 470.82.01
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.4.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.4.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.4.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.4.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.4.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.4.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.4.1
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                    x86_64
CPU op-mode(s):                  32-bit, 64-bit
Byte Order:                      Little Endian
Address sizes:                   46 bits physical, 57 bits virtual
CPU(s):                          128
On-line CPU(s) list:             0-127
Thread(s) per core:              2
Core(s) per socket:              32
Socket(s):                       2
NUMA node(s):                    2
Vendor ID:                       GenuineIntel
CPU family:                      6
Model:                           106
Model name:                      Intel(R) Xeon(R) Gold 6338 CPU @ 2.00GHz
Stepping:                        6
Frequency boost:                 enabled
CPU MHz:                         2600.000
CPU max MHz:                     3200.0000
CPU min MHz:                     800.0000
BogoMIPS:                        4000.00
Virtualization:                  VT-x
L1d cache:                       3 MiB
L1i cache:                       2 MiB
L2 cache:                        80 MiB
L3 cache:                        96 MiB
NUMA node0 CPU(s):               0-31,64-95
NUMA node1 CPU(s):               32-63,96-127
Vulnerability Itlb multihit:     Not affected
Vulnerability L1tf:              Not affected
Vulnerability Mds:               Not affected
Vulnerability Meltdown:          Not affected
Vulnerability Spec store bypass: Vulnerable
Vulnerability Spectre v1:        Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers
Vulnerability Spectre v2:        Vulnerable, IBPB: disabled, STIBP: disabled
Vulnerability Tsx async abort:   Not affected
Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq rdpid md_clear pconfig flush_l1d arch_capabilities

Versions of relevant libraries:
[pip3] numpy==1.25.1
[pip3] torch==2.0.1
[pip3] torchaudio==2.0.2
[pip3] torchvision==0.15.2
[conda] numpy                     1.25.1                   pypi_0    pypi
[conda] torch                     2.0.1                    pypi_0    pypi
[conda] torchaudio                2.0.2                    pypi_0    pypi
[conda] torchvision               0.15.2                   pypi_0    pypi
        Pillow (10.0.0)

Loading trainer: RPO
Loading dataset: StanfordCars
Reading split from /mnt/ul-nfs-2T/datasets/downstream_data/stanford_cars/split_zhou_StanfordCars.json
Loading preprocessed few-shot data from /mnt/ul-nfs-2T/datasets/downstream_data/stanford_cars/split_fewshot/shot_16-seed_3.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------------
Dataset    StanfordCars
# classes  98
# train_x  1,568
# val      392
# test     4,002
---------  ------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Parameters to be updated: {'prompt_learner.img_prompt', 'prompt_learner.text_prompt'}
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/rpo/base2new/train_base/stanford_cars/shots_16/RPO/main_K24/seed3/tensorboard)
epoch [1/15] batch [20/392] time 0.430 (0.604) data 0.000 (0.064) loss 1.5184 (1.6481) lr 1.0000e-05 eta 0:58:58
epoch [1/15] batch [40/392] time 0.393 (0.517) data 0.000 (0.032) loss 0.9329 (1.7322) lr 1.0000e-05 eta 0:50:18
epoch [1/15] batch [60/392] time 0.394 (0.480) data 0.000 (0.022) loss 2.8021 (1.7831) lr 1.0000e-05 eta 0:46:36
epoch [1/15] batch [80/392] time 0.378 (0.459) data 0.000 (0.016) loss 2.1465 (1.7227) lr 1.0000e-05 eta 0:44:22
epoch [1/15] batch [100/392] time 0.382 (0.447) data 0.000 (0.013) loss 1.5449 (1.6990) lr 1.0000e-05 eta 0:43:03
epoch [1/15] batch [120/392] time 0.383 (0.445) data 0.001 (0.011) loss 1.0671 (1.6568) lr 1.0000e-05 eta 0:42:43
epoch [1/15] batch [140/392] time 0.399 (0.449) data 0.000 (0.010) loss 0.9969 (1.6261) lr 1.0000e-05 eta 0:42:55
epoch [1/15] batch [160/392] time 0.393 (0.448) data 0.000 (0.008) loss 1.7074 (1.6171) lr 1.0000e-05 eta 0:42:43
epoch [1/15] batch [180/392] time 0.418 (0.455) data 0.000 (0.007) loss 1.4847 (1.6237) lr 1.0000e-05 eta 0:43:12
epoch [1/15] batch [200/392] time 0.494 (0.452) data 0.001 (0.007) loss 2.5680 (1.6304) lr 1.0000e-05 eta 0:42:45
epoch [1/15] batch [220/392] time 0.392 (0.450) data 0.000 (0.006) loss 2.2213 (1.6239) lr 1.0000e-05 eta 0:42:26
epoch [1/15] batch [240/392] time 0.379 (0.449) data 0.000 (0.006) loss 1.2591 (1.6252) lr 1.0000e-05 eta 0:42:11
epoch [1/15] batch [260/392] time 0.422 (0.448) data 0.000 (0.005) loss 1.3883 (1.6054) lr 1.0000e-05 eta 0:41:56
epoch [1/15] batch [280/392] time 0.471 (0.448) data 0.000 (0.005) loss 2.7809 (1.5990) lr 1.0000e-05 eta 0:41:50
epoch [1/15] batch [300/392] time 0.371 (0.447) data 0.001 (0.005) loss 1.1639 (1.5958) lr 1.0000e-05 eta 0:41:35
epoch [1/15] batch [320/392] time 0.394 (0.445) data 0.000 (0.004) loss 0.6892 (1.5896) lr 1.0000e-05 eta 0:41:15
epoch [1/15] batch [340/392] time 0.391 (0.444) data 0.000 (0.004) loss 0.8544 (1.6011) lr 1.0000e-05 eta 0:40:58
epoch [1/15] batch [360/392] time 0.442 (0.443) data 0.000 (0.004) loss 2.7215 (1.6006) lr 1.0000e-05 eta 0:40:45
epoch [1/15] batch [380/392] time 0.388 (0.441) data 0.000 (0.004) loss 0.8322 (1.5994) lr 1.0000e-05 eta 0:40:27
epoch [2/15] batch [20/392] time 0.473 (0.486) data 0.000 (0.039) loss 1.2014 (1.4245) lr 1.0000e-02 eta 0:44:19
epoch [2/15] batch [40/392] time 0.429 (0.465) data 0.000 (0.020) loss 1.6924 (1.5416) lr 1.0000e-02 eta 0:42:13
epoch [2/15] batch [60/392] time 0.474 (0.457) data 0.000 (0.013) loss 1.6742 (1.5814) lr 1.0000e-02 eta 0:41:19
epoch [2/15] batch [80/392] time 0.443 (0.454) data 0.000 (0.010) loss 1.0248 (1.5638) lr 1.0000e-02 eta 0:40:53
epoch [2/15] batch [100/392] time 0.400 (0.455) data 0.000 (0.008) loss 0.7637 (1.5860) lr 1.0000e-02 eta 0:40:49
epoch [2/15] batch [120/392] time 0.455 (0.453) data 0.000 (0.007) loss 1.4811 (1.5823) lr 1.0000e-02 eta 0:40:33
epoch [2/15] batch [140/392] time 0.556 (0.454) data 0.000 (0.006) loss 2.1335 (1.5762) lr 1.0000e-02 eta 0:40:28
epoch [2/15] batch [160/392] time 0.404 (0.456) data 0.000 (0.005) loss 0.9716 (1.5728) lr 1.0000e-02 eta 0:40:30
epoch [2/15] batch [180/392] time 0.508 (0.455) data 0.001 (0.005) loss 2.6204 (1.5468) lr 1.0000e-02 eta 0:40:15
epoch [2/15] batch [200/392] time 0.385 (0.454) data 0.000 (0.004) loss 0.9962 (1.5552) lr 1.0000e-02 eta 0:39:58
epoch [2/15] batch [220/392] time 0.382 (0.451) data 0.000 (0.004) loss 1.2132 (1.5632) lr 1.0000e-02 eta 0:39:38
epoch [2/15] batch [240/392] time 0.396 (0.453) data 0.000 (0.004) loss 1.0789 (1.5543) lr 1.0000e-02 eta 0:39:37
epoch [2/15] batch [260/392] time 0.382 (0.452) data 0.000 (0.003) loss 2.5984 (1.5519) lr 1.0000e-02 eta 0:39:24
epoch [2/15] batch [280/392] time 0.575 (0.455) data 0.001 (0.003) loss 1.4779 (1.5526) lr 1.0000e-02 eta 0:39:29
epoch [2/15] batch [300/392] time 0.388 (0.455) data 0.000 (0.003) loss 0.9215 (1.5515) lr 1.0000e-02 eta 0:39:22
epoch [2/15] batch [320/392] time 0.475 (0.457) data 0.001 (0.003) loss 1.2131 (1.5439) lr 1.0000e-02 eta 0:39:24
epoch [2/15] batch [340/392] time 0.387 (0.455) data 0.000 (0.003) loss 0.9525 (1.5420) lr 1.0000e-02 eta 0:39:00
epoch [2/15] batch [360/392] time 0.393 (0.453) data 0.001 (0.003) loss 1.6570 (1.5559) lr 1.0000e-02 eta 0:38:45
epoch [2/15] batch [380/392] time 0.386 (0.455) data 0.000 (0.002) loss 0.3678 (1.5441) lr 1.0000e-02 eta 0:38:43
epoch [3/15] batch [20/392] time 0.406 (0.492) data 0.000 (0.054) loss 0.6327 (1.5471) lr 9.8907e-03 eta 0:41:36
epoch [3/15] batch [40/392] time 0.395 (0.472) data 0.000 (0.027) loss 1.7176 (1.3989) lr 9.8907e-03 eta 0:39:44
epoch [3/15] batch [60/392] time 0.516 (0.469) data 0.000 (0.018) loss 1.1999 (1.4539) lr 9.8907e-03 eta 0:39:22
epoch [3/15] batch [80/392] time 0.444 (0.463) data 0.000 (0.014) loss 1.2831 (1.4039) lr 9.8907e-03 eta 0:38:41
epoch [3/15] batch [100/392] time 0.448 (0.469) data 0.000 (0.011) loss 2.0043 (1.4462) lr 9.8907e-03 eta 0:39:01
epoch [3/15] batch [120/392] time 0.475 (0.468) data 0.001 (0.009) loss 0.6079 (1.4111) lr 9.8907e-03 eta 0:38:50
epoch [3/15] batch [140/392] time 0.447 (0.465) data 0.000 (0.008) loss 1.2299 (1.4215) lr 9.8907e-03 eta 0:38:23
epoch [3/15] batch [160/392] time 0.513 (0.462) data 0.000 (0.007) loss 1.4809 (1.4534) lr 9.8907e-03 eta 0:38:01
epoch [3/15] batch [180/392] time 0.416 (0.459) data 0.000 (0.006) loss 2.9830 (1.4769) lr 9.8907e-03 eta 0:37:37
epoch [3/15] batch [200/392] time 0.471 (0.457) data 0.000 (0.006) loss 0.5816 (1.4913) lr 9.8907e-03 eta 0:37:17
epoch [3/15] batch [220/392] time 0.423 (0.455) data 0.000 (0.005) loss 1.9331 (1.4869) lr 9.8907e-03 eta 0:36:59
epoch [3/15] batch [240/392] time 0.409 (0.454) data 0.000 (0.005) loss 0.7282 (1.4951) lr 9.8907e-03 eta 0:36:42
epoch [3/15] batch [260/392] time 0.381 (0.453) data 0.000 (0.005) loss 1.2130 (1.4881) lr 9.8907e-03 eta 0:36:31
epoch [3/15] batch [280/392] time 0.543 (0.454) data 0.000 (0.004) loss 1.4281 (1.4996) lr 9.8907e-03 eta 0:36:25
epoch [3/15] batch [300/392] time 0.415 (0.455) data 0.000 (0.004) loss 1.4276 (1.4972) lr 9.8907e-03 eta 0:36:21
epoch [3/15] batch [320/392] time 0.480 (0.454) data 0.000 (0.004) loss 1.0076 (1.4874) lr 9.8907e-03 eta 0:36:10
epoch [3/15] batch [340/392] time 0.384 (0.454) data 0.000 (0.004) loss 3.4337 (1.4826) lr 9.8907e-03 eta 0:36:00
epoch [3/15] batch [360/392] time 0.549 (0.454) data 0.000 (0.003) loss 1.4408 (1.4749) lr 9.8907e-03 eta 0:35:50
epoch [3/15] batch [380/392] time 0.455 (0.452) data 0.000 (0.003) loss 1.2530 (1.4733) lr 9.8907e-03 eta 0:35:32
epoch [4/15] batch [20/392] time 0.390 (0.492) data 0.000 (0.050) loss 2.5840 (1.3939) lr 9.5677e-03 eta 0:38:23
epoch [4/15] batch [40/392] time 0.465 (0.480) data 0.000 (0.025) loss 1.4272 (1.4357) lr 9.5677e-03 eta 0:37:18
epoch [4/15] batch [60/392] time 0.391 (0.476) data 0.000 (0.017) loss 1.3101 (1.4928) lr 9.5677e-03 eta 0:36:50
epoch [4/15] batch [80/392] time 0.376 (0.468) data 0.000 (0.013) loss 2.3089 (1.4864) lr 9.5677e-03 eta 0:36:02
epoch [4/15] batch [100/392] time 0.570 (0.464) data 0.000 (0.010) loss 1.4526 (1.4385) lr 9.5677e-03 eta 0:35:35
epoch [4/15] batch [120/392] time 0.455 (0.467) data 0.000 (0.009) loss 1.2400 (1.4325) lr 9.5677e-03 eta 0:35:42
epoch [4/15] batch [140/392] time 0.436 (0.464) data 0.000 (0.007) loss 1.9711 (1.4436) lr 9.5677e-03 eta 0:35:15
epoch [4/15] batch [160/392] time 0.492 (0.461) data 0.000 (0.007) loss 1.6393 (1.4205) lr 9.5677e-03 eta 0:34:55
epoch [4/15] batch [180/392] time 0.406 (0.462) data 0.000 (0.006) loss 1.1540 (1.4305) lr 9.5677e-03 eta 0:34:50
epoch [4/15] batch [200/392] time 0.571 (0.463) data 0.000 (0.005) loss 1.6561 (1.4400) lr 9.5677e-03 eta 0:34:46
epoch [4/15] batch [220/392] time 0.376 (0.459) data 0.000 (0.005) loss 1.3056 (1.4349) lr 9.5677e-03 eta 0:34:17
epoch [4/15] batch [240/392] time 0.501 (0.457) data 0.000 (0.004) loss 1.5361 (1.4513) lr 9.5677e-03 eta 0:34:01
epoch [4/15] batch [260/392] time 0.389 (0.457) data 0.000 (0.004) loss 2.8797 (1.4568) lr 9.5677e-03 eta 0:33:49
epoch [4/15] batch [280/392] time 0.395 (0.456) data 0.000 (0.004) loss 1.0826 (1.4533) lr 9.5677e-03 eta 0:33:39
epoch [4/15] batch [300/392] time 0.602 (0.458) data 0.000 (0.004) loss 1.2139 (1.4521) lr 9.5677e-03 eta 0:33:37
epoch [4/15] batch [320/392] time 0.476 (0.459) data 0.000 (0.003) loss 2.0949 (1.4505) lr 9.5677e-03 eta 0:33:32
epoch [4/15] batch [340/392] time 0.503 (0.458) data 0.000 (0.003) loss 0.5275 (1.4458) lr 9.5677e-03 eta 0:33:18
epoch [4/15] batch [360/392] time 0.500 (0.457) data 0.000 (0.003) loss 0.3637 (1.4322) lr 9.5677e-03 eta 0:33:06
epoch [4/15] batch [380/392] time 0.499 (0.457) data 0.000 (0.003) loss 0.6862 (1.4333) lr 9.5677e-03 eta 0:32:54
epoch [5/15] batch [20/392] time 0.448 (0.506) data 0.000 (0.063) loss 1.7518 (1.6063) lr 9.0451e-03 eta 0:36:13
epoch [5/15] batch [40/392] time 0.497 (0.481) data 0.000 (0.032) loss 0.8289 (1.5137) lr 9.0451e-03 eta 0:34:14
epoch [5/15] batch [60/392] time 0.395 (0.478) data 0.000 (0.021) loss 0.8532 (1.4752) lr 9.0451e-03 eta 0:33:51
epoch [5/15] batch [80/392] time 0.534 (0.477) data 0.001 (0.016) loss 1.4447 (1.4846) lr 9.0451e-03 eta 0:33:39
epoch [5/15] batch [100/392] time 0.399 (0.472) data 0.000 (0.013) loss 1.4868 (1.4741) lr 9.0451e-03 eta 0:33:08
epoch [5/15] batch [120/392] time 0.506 (0.464) data 0.000 (0.011) loss 1.2616 (1.4412) lr 9.0451e-03 eta 0:32:25
epoch [5/15] batch [140/392] time 0.389 (0.460) data 0.000 (0.009) loss 0.9052 (1.4320) lr 9.0451e-03 eta 0:31:59
epoch [5/15] batch [160/392] time 0.385 (0.454) data 0.000 (0.008) loss 2.1641 (1.4374) lr 9.0451e-03 eta 0:31:26
epoch [5/15] batch [180/392] time 0.379 (0.447) data 0.000 (0.007) loss 0.8014 (1.4490) lr 9.0451e-03 eta 0:30:47
epoch [5/15] batch [200/392] time 0.380 (0.442) data 0.000 (0.007) loss 2.7958 (1.4699) lr 9.0451e-03 eta 0:30:15
epoch [5/15] batch [220/392] time 0.368 (0.436) data 0.000 (0.006) loss 1.9154 (1.4697) lr 9.0451e-03 eta 0:29:45
epoch [5/15] batch [240/392] time 0.401 (0.433) data 0.000 (0.006) loss 1.0959 (1.4565) lr 9.0451e-03 eta 0:29:24
epoch [5/15] batch [260/392] time 0.379 (0.433) data 0.000 (0.005) loss 1.1790 (1.4553) lr 9.0451e-03 eta 0:29:15
epoch [5/15] batch [280/392] time 0.555 (0.435) data 0.000 (0.005) loss 1.4497 (1.4407) lr 9.0451e-03 eta 0:29:14
epoch [5/15] batch [300/392] time 0.504 (0.440) data 0.000 (0.005) loss 0.9869 (1.4351) lr 9.0451e-03 eta 0:29:24
epoch [5/15] batch [320/392] time 0.457 (0.443) data 0.001 (0.004) loss 0.8660 (1.4459) lr 9.0451e-03 eta 0:29:29
epoch [5/15] batch [340/392] time 0.413 (0.444) data 0.000 (0.004) loss 0.9750 (1.4384) lr 9.0451e-03 eta 0:29:23
epoch [5/15] batch [360/392] time 0.404 (0.447) data 0.000 (0.004) loss 1.1377 (1.4489) lr 9.0451e-03 eta 0:29:25
epoch [5/15] batch [380/392] time 0.430 (0.447) data 0.000 (0.004) loss 2.1841 (1.4406) lr 9.0451e-03 eta 0:29:18
epoch [6/15] batch [20/392] time 0.386 (0.481) data 0.001 (0.052) loss 1.4103 (1.3080) lr 8.3457e-03 eta 0:31:17
epoch [6/15] batch [40/392] time 0.386 (0.469) data 0.000 (0.026) loss 2.0051 (1.3585) lr 8.3457e-03 eta 0:30:20
epoch [6/15] batch [60/392] time 0.391 (0.469) data 0.000 (0.018) loss 1.5686 (1.3404) lr 8.3457e-03 eta 0:30:08
epoch [6/15] batch [80/392] time 0.461 (0.471) data 0.000 (0.013) loss 1.6817 (1.3346) lr 8.3457e-03 eta 0:30:08
epoch [6/15] batch [100/392] time 0.381 (0.482) data 0.001 (0.011) loss 1.2196 (1.3992) lr 8.3457e-03 eta 0:30:42
epoch [6/15] batch [120/392] time 0.537 (0.479) data 0.000 (0.009) loss 0.6555 (1.4140) lr 8.3457e-03 eta 0:30:18
epoch [6/15] batch [140/392] time 0.477 (0.472) data 0.000 (0.008) loss 0.7249 (1.4117) lr 8.3457e-03 eta 0:29:43
epoch [6/15] batch [160/392] time 0.453 (0.468) data 0.000 (0.007) loss 1.1638 (1.4023) lr 8.3457e-03 eta 0:29:18
epoch [6/15] batch [180/392] time 0.639 (0.466) data 0.000 (0.006) loss 2.2213 (1.3936) lr 8.3457e-03 eta 0:29:03
epoch [6/15] batch [200/392] time 0.385 (0.463) data 0.000 (0.006) loss 1.1800 (1.4094) lr 8.3457e-03 eta 0:28:43
epoch [6/15] batch [220/392] time 0.516 (0.461) data 0.000 (0.005) loss 1.2538 (1.4012) lr 8.3457e-03 eta 0:28:26
epoch [6/15] batch [240/392] time 0.509 (0.461) data 0.000 (0.005) loss 1.9153 (1.4101) lr 8.3457e-03 eta 0:28:16
epoch [6/15] batch [260/392] time 0.434 (0.460) data 0.000 (0.004) loss 1.1039 (1.3901) lr 8.3457e-03 eta 0:28:04
epoch [6/15] batch [280/392] time 0.466 (0.459) data 0.000 (0.004) loss 2.4109 (1.3987) lr 8.3457e-03 eta 0:27:49
epoch [6/15] batch [300/392] time 0.384 (0.456) data 0.000 (0.004) loss 0.8170 (1.3944) lr 8.3457e-03 eta 0:27:30
epoch [6/15] batch [320/392] time 0.378 (0.455) data 0.000 (0.004) loss 1.1183 (1.3921) lr 8.3457e-03 eta 0:27:17
epoch [6/15] batch [340/392] time 0.439 (0.455) data 0.000 (0.003) loss 0.9451 (1.3853) lr 8.3457e-03 eta 0:27:08
epoch [6/15] batch [360/392] time 0.377 (0.455) data 0.000 (0.003) loss 1.2093 (1.3898) lr 8.3457e-03 eta 0:26:58
epoch [6/15] batch [380/392] time 0.364 (0.455) data 0.000 (0.003) loss 0.2266 (1.4013) lr 8.3457e-03 eta 0:26:51
epoch [7/15] batch [20/392] time 0.521 (0.532) data 0.000 (0.050) loss 2.3451 (1.4414) lr 7.5000e-03 eta 0:31:07
epoch [7/15] batch [40/392] time 0.442 (0.489) data 0.000 (0.025) loss 1.1021 (1.3627) lr 7.5000e-03 eta 0:28:25
epoch [7/15] batch [60/392] time 0.479 (0.486) data 0.000 (0.017) loss 2.6510 (1.3876) lr 7.5000e-03 eta 0:28:05
epoch [7/15] batch [80/392] time 0.445 (0.478) data 0.000 (0.013) loss 1.5730 (1.4263) lr 7.5000e-03 eta 0:27:27
epoch [7/15] batch [100/392] time 0.420 (0.477) data 0.000 (0.010) loss 0.7702 (1.3856) lr 7.5000e-03 eta 0:27:15
epoch [7/15] batch [120/392] time 0.521 (0.480) data 0.000 (0.009) loss 2.3599 (1.3914) lr 7.5000e-03 eta 0:27:15
epoch [7/15] batch [140/392] time 0.459 (0.479) data 0.000 (0.008) loss 1.5156 (1.3719) lr 7.5000e-03 eta 0:27:02
epoch [7/15] batch [160/392] time 0.481 (0.471) data 0.000 (0.007) loss 0.8946 (1.3937) lr 7.5000e-03 eta 0:26:26
epoch [7/15] batch [180/392] time 0.405 (0.466) data 0.000 (0.006) loss 1.7123 (1.4248) lr 7.5000e-03 eta 0:26:01
epoch [7/15] batch [200/392] time 0.441 (0.463) data 0.000 (0.005) loss 2.1592 (1.4343) lr 7.5000e-03 eta 0:25:41
epoch [7/15] batch [220/392] time 0.410 (0.462) data 0.000 (0.005) loss 1.3545 (1.4213) lr 7.5000e-03 eta 0:25:29
epoch [7/15] batch [240/392] time 0.420 (0.462) data 0.000 (0.005) loss 0.4899 (1.4296) lr 7.5000e-03 eta 0:25:20
epoch [7/15] batch [260/392] time 0.412 (0.461) data 0.000 (0.004) loss 0.6567 (1.4370) lr 7.5000e-03 eta 0:25:07
epoch [7/15] batch [280/392] time 0.480 (0.459) data 0.000 (0.004) loss 2.3102 (1.4433) lr 7.5000e-03 eta 0:24:50
epoch [7/15] batch [300/392] time 0.459 (0.458) data 0.000 (0.004) loss 0.6006 (1.4322) lr 7.5000e-03 eta 0:24:36
epoch [7/15] batch [320/392] time 0.413 (0.457) data 0.001 (0.003) loss 2.0072 (1.4374) lr 7.5000e-03 eta 0:24:26
epoch [7/15] batch [340/392] time 0.587 (0.456) data 0.000 (0.003) loss 0.7466 (1.4443) lr 7.5000e-03 eta 0:24:13
epoch [7/15] batch [360/392] time 0.477 (0.457) data 0.000 (0.003) loss 1.0976 (1.4354) lr 7.5000e-03 eta 0:24:06
epoch [7/15] batch [380/392] time 0.407 (0.456) data 0.000 (0.003) loss 1.0604 (1.4323) lr 7.5000e-03 eta 0:23:54
epoch [8/15] batch [20/392] time 0.395 (0.508) data 0.000 (0.058) loss 1.0048 (1.4801) lr 6.5451e-03 eta 0:26:21
epoch [8/15] batch [40/392] time 0.384 (0.466) data 0.000 (0.029) loss 1.3725 (1.2699) lr 6.5451e-03 eta 0:24:01
epoch [8/15] batch [60/392] time 0.434 (0.461) data 0.000 (0.019) loss 2.2653 (1.3530) lr 6.5451e-03 eta 0:23:36
epoch [8/15] batch [80/392] time 0.385 (0.463) data 0.000 (0.015) loss 2.1375 (1.4521) lr 6.5451e-03 eta 0:23:34
epoch [8/15] batch [100/392] time 0.496 (0.461) data 0.000 (0.012) loss 2.4878 (1.4329) lr 6.5451e-03 eta 0:23:20
epoch [8/15] batch [120/392] time 0.401 (0.463) data 0.001 (0.010) loss 1.1762 (1.4288) lr 6.5451e-03 eta 0:23:17
epoch [8/15] batch [140/392] time 0.400 (0.463) data 0.000 (0.009) loss 1.0395 (1.4308) lr 6.5451e-03 eta 0:23:06
epoch [8/15] batch [160/392] time 0.580 (0.459) data 0.000 (0.008) loss 0.9638 (1.4458) lr 6.5451e-03 eta 0:22:46
epoch [8/15] batch [180/392] time 0.493 (0.458) data 0.000 (0.007) loss 1.8178 (1.4361) lr 6.5451e-03 eta 0:22:33
epoch [8/15] batch [200/392] time 0.404 (0.455) data 0.000 (0.006) loss 1.1986 (1.4377) lr 6.5451e-03 eta 0:22:16
epoch [8/15] batch [220/392] time 0.429 (0.455) data 0.000 (0.006) loss 1.8021 (1.4282) lr 6.5451e-03 eta 0:22:06
epoch [8/15] batch [240/392] time 0.482 (0.457) data 0.000 (0.005) loss 0.8335 (1.4235) lr 6.5451e-03 eta 0:22:02
epoch [8/15] batch [260/392] time 0.394 (0.457) data 0.000 (0.005) loss 1.8933 (1.4321) lr 6.5451e-03 eta 0:21:53
epoch [8/15] batch [280/392] time 0.507 (0.457) data 0.000 (0.004) loss 0.9604 (1.4280) lr 6.5451e-03 eta 0:21:45
epoch [8/15] batch [300/392] time 0.473 (0.455) data 0.000 (0.004) loss 2.4420 (1.4264) lr 6.5451e-03 eta 0:21:30
epoch [8/15] batch [320/392] time 0.503 (0.456) data 0.000 (0.004) loss 1.5049 (1.4274) lr 6.5451e-03 eta 0:21:23
epoch [8/15] batch [340/392] time 0.593 (0.456) data 0.000 (0.004) loss 2.0434 (1.4366) lr 6.5451e-03 eta 0:21:15
epoch [8/15] batch [360/392] time 0.406 (0.458) data 0.000 (0.004) loss 0.7896 (1.4396) lr 6.5451e-03 eta 0:21:11
epoch [8/15] batch [380/392] time 0.393 (0.458) data 0.000 (0.003) loss 2.2178 (1.4303) lr 6.5451e-03 eta 0:21:03
epoch [9/15] batch [20/392] time 0.472 (0.495) data 0.000 (0.047) loss 1.5122 (1.4220) lr 5.5226e-03 eta 0:22:28
epoch [9/15] batch [40/392] time 0.447 (0.464) data 0.000 (0.024) loss 1.9824 (1.4159) lr 5.5226e-03 eta 0:20:54
epoch [9/15] batch [60/392] time 0.529 (0.459) data 0.000 (0.016) loss 0.5180 (1.4388) lr 5.5226e-03 eta 0:20:31
epoch [9/15] batch [80/392] time 0.495 (0.456) data 0.000 (0.012) loss 2.3330 (1.4516) lr 5.5226e-03 eta 0:20:15
epoch [9/15] batch [100/392] time 0.387 (0.456) data 0.000 (0.010) loss 0.9656 (1.4133) lr 5.5226e-03 eta 0:20:04
epoch [9/15] batch [120/392] time 0.467 (0.459) data 0.000 (0.008) loss 0.9721 (1.4001) lr 5.5226e-03 eta 0:20:05
epoch [9/15] batch [140/392] time 0.634 (0.456) data 0.000 (0.007) loss 1.6950 (1.3739) lr 5.5226e-03 eta 0:19:46
epoch [9/15] batch [160/392] time 0.417 (0.456) data 0.000 (0.006) loss 2.0861 (1.3712) lr 5.5226e-03 eta 0:19:38
epoch [9/15] batch [180/392] time 0.388 (0.455) data 0.000 (0.006) loss 2.3451 (1.3766) lr 5.5226e-03 eta 0:19:25
epoch [9/15] batch [200/392] time 0.387 (0.455) data 0.001 (0.005) loss 0.8325 (1.3557) lr 5.5226e-03 eta 0:19:18
epoch [9/15] batch [220/392] time 0.571 (0.458) data 0.000 (0.005) loss 0.6028 (1.3410) lr 5.5226e-03 eta 0:19:15
epoch [9/15] batch [240/392] time 0.479 (0.458) data 0.000 (0.004) loss 1.5930 (1.3439) lr 5.5226e-03 eta 0:19:06
epoch [9/15] batch [260/392] time 0.413 (0.458) data 0.000 (0.004) loss 2.3263 (1.3481) lr 5.5226e-03 eta 0:18:57
epoch [9/15] batch [280/392] time 0.535 (0.459) data 0.000 (0.004) loss 0.7737 (1.3403) lr 5.5226e-03 eta 0:18:52
epoch [9/15] batch [300/392] time 0.535 (0.457) data 0.000 (0.003) loss 0.7136 (1.3328) lr 5.5226e-03 eta 0:18:37
epoch [9/15] batch [320/392] time 0.484 (0.459) data 0.000 (0.003) loss 0.7468 (1.3344) lr 5.5226e-03 eta 0:18:31
epoch [9/15] batch [340/392] time 0.414 (0.460) data 0.000 (0.003) loss 1.5794 (1.3268) lr 5.5226e-03 eta 0:18:25
epoch [9/15] batch [360/392] time 0.389 (0.461) data 0.000 (0.003) loss 1.6943 (1.3479) lr 5.5226e-03 eta 0:18:18
epoch [9/15] batch [380/392] time 0.446 (0.461) data 0.000 (0.003) loss 0.9137 (1.3444) lr 5.5226e-03 eta 0:18:09
epoch [10/15] batch [20/392] time 0.469 (0.497) data 0.000 (0.042) loss 2.6823 (1.4695) lr 4.4774e-03 eta 0:19:19
epoch [10/15] batch [40/392] time 0.481 (0.472) data 0.001 (0.021) loss 0.6213 (1.3347) lr 4.4774e-03 eta 0:18:10
epoch [10/15] batch [60/392] time 0.656 (0.460) data 0.000 (0.014) loss 1.3488 (1.2962) lr 4.4774e-03 eta 0:17:34
epoch [10/15] batch [80/392] time 0.379 (0.452) data 0.000 (0.011) loss 4.2501 (1.3478) lr 4.4774e-03 eta 0:17:07
epoch [10/15] batch [100/392] time 0.557 (0.450) data 0.000 (0.009) loss 2.2284 (1.3461) lr 4.4774e-03 eta 0:16:54
epoch [10/15] batch [120/392] time 0.440 (0.453) data 0.000 (0.007) loss 1.4417 (1.3393) lr 4.4774e-03 eta 0:16:51
epoch [10/15] batch [140/392] time 0.474 (0.457) data 0.000 (0.006) loss 1.0037 (1.3290) lr 4.4774e-03 eta 0:16:51
epoch [10/15] batch [160/392] time 0.379 (0.455) data 0.000 (0.006) loss 0.7439 (1.3289) lr 4.4774e-03 eta 0:16:38
epoch [10/15] batch [180/392] time 0.393 (0.454) data 0.000 (0.005) loss 1.6405 (1.3441) lr 4.4774e-03 eta 0:16:26
epoch [10/15] batch [200/392] time 0.554 (0.457) data 0.000 (0.005) loss 1.5896 (1.3358) lr 4.4774e-03 eta 0:16:23
epoch [10/15] batch [220/392] time 0.419 (0.458) data 0.000 (0.004) loss 2.2465 (1.3371) lr 4.4774e-03 eta 0:16:15
epoch [10/15] batch [240/392] time 0.444 (0.458) data 0.000 (0.004) loss 1.4442 (1.3319) lr 4.4774e-03 eta 0:16:08
epoch [10/15] batch [260/392] time 0.485 (0.458) data 0.000 (0.004) loss 0.7907 (1.3223) lr 4.4774e-03 eta 0:15:58
epoch [10/15] batch [280/392] time 0.376 (0.456) data 0.000 (0.003) loss 3.1127 (1.3257) lr 4.4774e-03 eta 0:15:44
epoch [10/15] batch [300/392] time 0.404 (0.455) data 0.000 (0.003) loss 1.2257 (1.3210) lr 4.4774e-03 eta 0:15:33
epoch [10/15] batch [320/392] time 0.608 (0.455) data 0.000 (0.003) loss 0.6432 (1.3174) lr 4.4774e-03 eta 0:15:24
epoch [10/15] batch [340/392] time 0.453 (0.455) data 0.000 (0.003) loss 1.2311 (1.3205) lr 4.4774e-03 eta 0:15:14
epoch [10/15] batch [360/392] time 0.508 (0.455) data 0.000 (0.003) loss 0.9447 (1.3286) lr 4.4774e-03 eta 0:15:05
epoch [10/15] batch [380/392] time 0.397 (0.455) data 0.000 (0.003) loss 1.2698 (1.3328) lr 4.4774e-03 eta 0:14:57
epoch [11/15] batch [20/392] time 0.424 (0.511) data 0.001 (0.060) loss 1.8059 (1.2699) lr 3.4549e-03 eta 0:16:31
epoch [11/15] batch [40/392] time 0.453 (0.476) data 0.000 (0.030) loss 1.0712 (1.3890) lr 3.4549e-03 eta 0:15:14
epoch [11/15] batch [60/392] time 0.440 (0.462) data 0.000 (0.020) loss 0.7256 (1.3406) lr 3.4549e-03 eta 0:14:37
epoch [11/15] batch [80/392] time 0.474 (0.461) data 0.000 (0.015) loss 1.2493 (1.3384) lr 3.4549e-03 eta 0:14:26
epoch [11/15] batch [100/392] time 0.417 (0.462) data 0.000 (0.012) loss 0.6598 (1.3445) lr 3.4549e-03 eta 0:14:18
epoch [11/15] batch [120/392] time 0.501 (0.454) data 0.000 (0.010) loss 1.0111 (1.3452) lr 3.4549e-03 eta 0:13:54
epoch [11/15] batch [140/392] time 0.377 (0.446) data 0.000 (0.009) loss 0.8568 (1.3275) lr 3.4549e-03 eta 0:13:31
epoch [11/15] batch [160/392] time 0.373 (0.441) data 0.000 (0.008) loss 1.5093 (1.3139) lr 3.4549e-03 eta 0:13:13
epoch [11/15] batch [180/392] time 0.379 (0.440) data 0.000 (0.007) loss 1.3198 (1.3200) lr 3.4549e-03 eta 0:13:03
epoch [11/15] batch [200/392] time 0.382 (0.441) data 0.001 (0.006) loss 1.0410 (1.3236) lr 3.4549e-03 eta 0:12:55
epoch [11/15] batch [220/392] time 0.564 (0.440) data 0.000 (0.006) loss 0.7188 (1.3089) lr 3.4549e-03 eta 0:12:45
epoch [11/15] batch [240/392] time 0.419 (0.438) data 0.000 (0.005) loss 0.8860 (1.2936) lr 3.4549e-03 eta 0:12:33
epoch [11/15] batch [260/392] time 0.383 (0.439) data 0.000 (0.005) loss 0.4813 (1.2866) lr 3.4549e-03 eta 0:12:26
epoch [11/15] batch [280/392] time 0.460 (0.440) data 0.000 (0.005) loss 0.7896 (1.2793) lr 3.4549e-03 eta 0:12:18
epoch [11/15] batch [300/392] time 0.386 (0.442) data 0.000 (0.004) loss 0.9222 (1.2720) lr 3.4549e-03 eta 0:12:14
epoch [11/15] batch [320/392] time 0.364 (0.442) data 0.000 (0.004) loss 0.9284 (1.2785) lr 3.4549e-03 eta 0:12:04
epoch [11/15] batch [340/392] time 0.552 (0.442) data 0.000 (0.004) loss 1.6575 (1.2850) lr 3.4549e-03 eta 0:11:56
epoch [11/15] batch [360/392] time 0.383 (0.442) data 0.000 (0.004) loss 0.7648 (1.2943) lr 3.4549e-03 eta 0:11:47
epoch [11/15] batch [380/392] time 0.508 (0.444) data 0.000 (0.004) loss 1.4861 (1.2874) lr 3.4549e-03 eta 0:11:41
epoch [12/15] batch [20/392] time 0.369 (0.890) data 0.000 (0.471) loss 1.1093 (1.5247) lr 2.5000e-03 eta 0:22:57
epoch [12/15] batch [40/392] time 0.390 (0.643) data 0.000 (0.236) loss 1.3009 (1.3951) lr 2.5000e-03 eta 0:16:21
epoch [12/15] batch [60/392] time 0.386 (0.559) data 0.000 (0.157) loss 0.9383 (1.3316) lr 2.5000e-03 eta 0:14:02
epoch [12/15] batch [80/392] time 0.461 (0.528) data 0.000 (0.118) loss 1.1579 (1.3464) lr 2.5000e-03 eta 0:13:05
epoch [12/15] batch [100/392] time 0.559 (0.517) data 0.000 (0.094) loss 1.7157 (1.3659) lr 2.5000e-03 eta 0:12:38
epoch [12/15] batch [120/392] time 0.473 (0.505) data 0.000 (0.079) loss 0.8267 (1.3351) lr 2.5000e-03 eta 0:12:11
epoch [12/15] batch [140/392] time 0.513 (0.503) data 0.000 (0.068) loss 0.2829 (1.3002) lr 2.5000e-03 eta 0:11:58
epoch [12/15] batch [160/392] time 0.414 (0.495) data 0.000 (0.059) loss 1.3462 (1.3017) lr 2.5000e-03 eta 0:11:36
epoch [12/15] batch [180/392] time 0.551 (0.492) data 0.000 (0.053) loss 0.9089 (1.3000) lr 2.5000e-03 eta 0:11:22
epoch [12/15] batch [200/392] time 0.516 (0.490) data 0.000 (0.047) loss 1.3440 (1.2875) lr 2.5000e-03 eta 0:11:09
epoch [12/15] batch [220/392] time 0.585 (0.490) data 0.000 (0.043) loss 1.9437 (1.2865) lr 2.5000e-03 eta 0:11:00
epoch [12/15] batch [240/392] time 0.383 (0.490) data 0.000 (0.040) loss 1.5026 (1.2824) lr 2.5000e-03 eta 0:10:50
epoch [12/15] batch [260/392] time 0.411 (0.486) data 0.000 (0.037) loss 0.8597 (1.2821) lr 2.5000e-03 eta 0:10:36
epoch [12/15] batch [280/392] time 0.648 (0.487) data 0.000 (0.034) loss 1.7498 (1.2730) lr 2.5000e-03 eta 0:10:27
epoch [12/15] batch [300/392] time 0.388 (0.483) data 0.000 (0.032) loss 1.1148 (1.2784) lr 2.5000e-03 eta 0:10:12
epoch [12/15] batch [320/392] time 0.441 (0.483) data 0.000 (0.030) loss 1.9233 (1.2764) lr 2.5000e-03 eta 0:10:02
epoch [12/15] batch [340/392] time 0.409 (0.482) data 0.000 (0.028) loss 1.6023 (1.2682) lr 2.5000e-03 eta 0:09:51
epoch [12/15] batch [360/392] time 0.406 (0.479) data 0.000 (0.026) loss 1.3469 (1.2827) lr 2.5000e-03 eta 0:09:39
epoch [12/15] batch [380/392] time 0.476 (0.478) data 0.000 (0.025) loss 1.6267 (1.2848) lr 2.5000e-03 eta 0:09:28
epoch [13/15] batch [20/392] time 0.418 (0.498) data 0.000 (0.036) loss 0.6237 (1.0107) lr 1.6543e-03 eta 0:09:35
epoch [13/15] batch [40/392] time 0.507 (0.469) data 0.000 (0.018) loss 1.3275 (1.1449) lr 1.6543e-03 eta 0:08:52
epoch [13/15] batch [60/392] time 0.395 (0.458) data 0.000 (0.012) loss 3.0364 (1.2397) lr 1.6543e-03 eta 0:08:31
epoch [13/15] batch [80/392] time 0.524 (0.454) data 0.000 (0.009) loss 0.8084 (1.2638) lr 1.6543e-03 eta 0:08:17
epoch [13/15] batch [100/392] time 0.401 (0.458) data 0.000 (0.008) loss 1.1524 (1.2642) lr 1.6543e-03 eta 0:08:12
epoch [13/15] batch [120/392] time 0.416 (0.458) data 0.000 (0.006) loss 0.8941 (1.2477) lr 1.6543e-03 eta 0:08:03
epoch [13/15] batch [140/392] time 0.545 (0.460) data 0.000 (0.006) loss 0.8049 (1.2228) lr 1.6543e-03 eta 0:07:56
epoch [13/15] batch [160/392] time 0.394 (0.458) data 0.000 (0.005) loss 0.5516 (1.2169) lr 1.6543e-03 eta 0:07:45
epoch [13/15] batch [180/392] time 0.461 (0.458) data 0.000 (0.004) loss 1.4692 (1.2326) lr 1.6543e-03 eta 0:07:36
epoch [13/15] batch [200/392] time 0.528 (0.457) data 0.000 (0.004) loss 1.5727 (1.2517) lr 1.6543e-03 eta 0:07:25
epoch [13/15] batch [220/392] time 0.472 (0.455) data 0.000 (0.004) loss 0.7487 (1.2415) lr 1.6543e-03 eta 0:07:15
epoch [13/15] batch [240/392] time 0.384 (0.456) data 0.000 (0.003) loss 1.9055 (1.2356) lr 1.6543e-03 eta 0:07:06
epoch [13/15] batch [260/392] time 0.418 (0.456) data 0.000 (0.003) loss 2.4098 (1.2437) lr 1.6543e-03 eta 0:06:57
epoch [13/15] batch [280/392] time 0.429 (0.457) data 0.000 (0.003) loss 0.8850 (1.2495) lr 1.6543e-03 eta 0:06:49
epoch [13/15] batch [300/392] time 0.366 (0.455) data 0.000 (0.003) loss 1.3807 (1.2416) lr 1.6543e-03 eta 0:06:38
epoch [13/15] batch [320/392] time 0.390 (0.456) data 0.000 (0.003) loss 0.1753 (1.2462) lr 1.6543e-03 eta 0:06:30
epoch [13/15] batch [340/392] time 0.447 (0.454) data 0.000 (0.002) loss 1.0078 (1.2534) lr 1.6543e-03 eta 0:06:19
epoch [13/15] batch [360/392] time 0.461 (0.454) data 0.000 (0.002) loss 2.9533 (1.2471) lr 1.6543e-03 eta 0:06:10
epoch [13/15] batch [380/392] time 0.467 (0.454) data 0.000 (0.002) loss 1.4579 (1.2582) lr 1.6543e-03 eta 0:06:01
epoch [14/15] batch [20/392] time 0.762 (0.509) data 0.000 (0.043) loss 0.7672 (1.2048) lr 9.5492e-04 eta 0:06:28
epoch [14/15] batch [40/392] time 0.536 (0.480) data 0.000 (0.022) loss 2.2088 (1.2399) lr 9.5492e-04 eta 0:05:57
epoch [14/15] batch [60/392] time 0.403 (0.464) data 0.000 (0.015) loss 1.8699 (1.2228) lr 9.5492e-04 eta 0:05:36
epoch [14/15] batch [80/392] time 0.522 (0.473) data 0.000 (0.011) loss 1.3185 (1.2497) lr 9.5492e-04 eta 0:05:32
epoch [14/15] batch [100/392] time 0.394 (0.468) data 0.000 (0.009) loss 0.6489 (1.2787) lr 9.5492e-04 eta 0:05:20
epoch [14/15] batch [120/392] time 0.425 (0.461) data 0.000 (0.007) loss 0.9143 (1.2265) lr 9.5492e-04 eta 0:05:06
epoch [14/15] batch [140/392] time 0.381 (0.455) data 0.000 (0.006) loss 0.9658 (1.1866) lr 9.5492e-04 eta 0:04:52
epoch [14/15] batch [160/392] time 0.381 (0.448) data 0.001 (0.006) loss 1.2458 (1.1704) lr 9.5492e-04 eta 0:04:39
epoch [14/15] batch [180/392] time 0.428 (0.444) data 0.000 (0.005) loss 0.8205 (1.1685) lr 9.5492e-04 eta 0:04:28
epoch [14/15] batch [200/392] time 0.511 (0.444) data 0.001 (0.005) loss 2.0816 (1.1708) lr 9.5492e-04 eta 0:04:19
epoch [14/15] batch [220/392] time 0.388 (0.443) data 0.000 (0.004) loss 0.7685 (1.1841) lr 9.5492e-04 eta 0:04:09
epoch [14/15] batch [240/392] time 0.481 (0.445) data 0.000 (0.004) loss 2.4256 (1.1930) lr 9.5492e-04 eta 0:04:02
epoch [14/15] batch [260/392] time 0.474 (0.445) data 0.001 (0.004) loss 0.7115 (1.2067) lr 9.5492e-04 eta 0:03:53
epoch [14/15] batch [280/392] time 0.458 (0.445) data 0.000 (0.003) loss 1.1461 (1.1955) lr 9.5492e-04 eta 0:03:44
epoch [14/15] batch [300/392] time 0.400 (0.446) data 0.000 (0.003) loss 1.7242 (1.1973) lr 9.5492e-04 eta 0:03:35
epoch [14/15] batch [320/392] time 0.548 (0.446) data 0.001 (0.003) loss 1.7216 (1.2061) lr 9.5492e-04 eta 0:03:27
epoch [14/15] batch [340/392] time 0.392 (0.447) data 0.000 (0.003) loss 2.0481 (1.2049) lr 9.5492e-04 eta 0:03:18
epoch [14/15] batch [360/392] time 0.474 (0.448) data 0.000 (0.003) loss 1.1588 (1.1938) lr 9.5492e-04 eta 0:03:09
epoch [14/15] batch [380/392] time 0.523 (0.450) data 0.000 (0.003) loss 0.7778 (1.1895) lr 9.5492e-04 eta 0:03:01
epoch [15/15] batch [20/392] time 0.454 (0.551) data 0.000 (0.044) loss 1.5285 (1.2469) lr 4.3227e-04 eta 0:03:25
epoch [15/15] batch [40/392] time 0.422 (0.486) data 0.000 (0.022) loss 0.8810 (1.1601) lr 4.3227e-04 eta 0:02:51
epoch [15/15] batch [60/392] time 0.490 (0.475) data 0.000 (0.015) loss 1.0684 (1.1987) lr 4.3227e-04 eta 0:02:37
epoch [15/15] batch [80/392] time 0.507 (0.473) data 0.000 (0.011) loss 0.8615 (1.1784) lr 4.3227e-04 eta 0:02:27
epoch [15/15] batch [100/392] time 0.441 (0.472) data 0.000 (0.009) loss 0.5753 (1.1934) lr 4.3227e-04 eta 0:02:17
epoch [15/15] batch [120/392] time 0.379 (0.473) data 0.000 (0.008) loss 1.6482 (1.2176) lr 4.3227e-04 eta 0:02:08
epoch [15/15] batch [140/392] time 0.482 (0.468) data 0.001 (0.007) loss 1.1088 (1.2293) lr 4.3227e-04 eta 0:01:57
epoch [15/15] batch [160/392] time 0.550 (0.465) data 0.000 (0.006) loss 1.1058 (1.2251) lr 4.3227e-04 eta 0:01:47
epoch [15/15] batch [180/392] time 0.395 (0.461) data 0.000 (0.005) loss 1.2915 (1.2394) lr 4.3227e-04 eta 0:01:37
epoch [15/15] batch [200/392] time 0.456 (0.461) data 0.000 (0.005) loss 0.6911 (1.2523) lr 4.3227e-04 eta 0:01:28
epoch [15/15] batch [220/392] time 0.524 (0.460) data 0.000 (0.004) loss 1.8392 (1.2384) lr 4.3227e-04 eta 0:01:19
epoch [15/15] batch [240/392] time 0.433 (0.460) data 0.000 (0.004) loss 0.8519 (1.2286) lr 4.3227e-04 eta 0:01:09
epoch [15/15] batch [260/392] time 0.532 (0.458) data 0.000 (0.004) loss 0.7797 (1.2186) lr 4.3227e-04 eta 0:01:00
epoch [15/15] batch [280/392] time 0.590 (0.457) data 0.000 (0.003) loss 1.1896 (1.2345) lr 4.3227e-04 eta 0:00:51
epoch [15/15] batch [300/392] time 0.427 (0.457) data 0.000 (0.003) loss 1.3727 (1.2229) lr 4.3227e-04 eta 0:00:42
epoch [15/15] batch [320/392] time 0.478 (0.458) data 0.000 (0.003) loss 0.2745 (1.2214) lr 4.3227e-04 eta 0:00:32
epoch [15/15] batch [340/392] time 0.389 (0.458) data 0.000 (0.003) loss 1.4543 (1.2307) lr 4.3227e-04 eta 0:00:23
epoch [15/15] batch [360/392] time 0.490 (0.458) data 0.000 (0.003) loss 1.6508 (1.2522) lr 4.3227e-04 eta 0:00:14
epoch [15/15] batch [380/392] time 0.487 (0.457) data 0.000 (0.003) loss 0.4328 (1.2400) lr 4.3227e-04 eta 0:00:05
Checkpoint saved to output/rpo/base2new/train_base/stanford_cars/shots_16/RPO/main_K24/seed3/prompt_learner/model.pth.tar-15
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
=> result
* total: 4,002
* correct: 2,977
* accuracy: 74.4%
* error: 25.6%
* macro_f1: 73.9%
Elapsed: 0:54:30
